{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic-regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "uT4d8uPWmWBy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Notação\n",
        "Notação matemática para deep learning.\n",
        "\n",
        "## Tamanhos\n",
        "| Notação | Significado |\n",
        "| -------------- | ------------------ |\n",
        "| $m$ | Número de exemplos no dataset |\n",
        "| $n_{x}$ | Tamanho da entrada  |\n",
        "| $n_{y}$ | Tamanho da saída (ou número de classes)  |\n",
        "| $n_{h}^{[l]}$ | Número de unidades ocultas da camada $l^{th}$ |\n",
        "| $L$ | Número de camadas na rede |\n",
        "\n",
        "**Observação**: Em um _loop_, é possível denotar $n_{x} = n_{h}^{[0]}$ e $n_{y} = n_{h}^{numero\\_de\\_camadas + 1}$.\n",
        "\n",
        "## Objetos\n",
        "| Notação | Significado |\n",
        "| -------------- | ------------------ |\n",
        "| $X \\in \\mathbb{R}^{n_{x} \\times m}$ | Matriz de entrada |\n",
        "| $x^{(i)} \\in \\mathbb{R}^{n_{x}}$ | Representa o vetor da columa $i^{th}$  |\n",
        "| $Y \\in \\mathbb{R}^{n_{y} \\times m}$ | Matriz de rótulos |\n",
        "| $y^{(i)} \\in \\mathbb{R}^{n_{y}}$ | Rótulo de saída para o exemplo $i^{th}$  |\n",
        "| $W^{[l]} \\in \\mathbb{R}^{numero\\_de\\_unidades\\_na\\_proxima\\_camada \\times numero\\_de\\_unidades\\_na\\_camada\\_anterior}$ | É a matriz de peso, $[l]$ indica a camada |\n",
        "| $b^{[l]} \\in \\mathbb{R}^{numero\\_de\\_unidades\\_na\\_proxima\\_camada}$ | É o vetor de viés na camada $l^{th}$ |\n",
        "| $\\hat{y} \\in \\mathbb{R}^{n_{y}}$ | É o vetor de saída previsto. Também pode ser denotado como $a^{[L]}$, onde $L$ é o número de camadas na rede |\n",
        "\\begin{equation}\n",
        "X_{n_{x} \\times m} =\n",
        "  \\begin{bmatrix}\n",
        "    \\vdots & \\vdots &\\vdots &   & \\vdots \\\\\n",
        "    x^{(1)} & x^{(2)} & x^{(3)} & \\dots  & x^{(m)} \\\\\n",
        "    \\vdots & \\vdots &\\vdots &   & \\vdots\n",
        "  \\end{bmatrix}\n",
        "Y_{1 \\times m} =\n",
        "  \\begin{bmatrix}\n",
        "    y^{(1)} & y^{(2)} & y^{(3)} & \\dots  & y^{(m)}\n",
        "  \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "## Outros\n",
        "\n",
        "| Notação | Significado |\n",
        "| -------------- | ------------------ |\n",
        "| $(x^{(1)}, y^{(1)})$ | Exemplo de treinamento único |"
      ]
    },
    {
      "metadata": {
        "id": "BHb1xaUR4lAt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Regressão Logística\n",
        "Regressão logística é um algoritmo de classificação binária, isso quer dizer que é um algoritmo de aprendizagem que usamos quando os rótulos de saída $Y$ em um problema de aprendizado supervisionado são ou $0$ ou $1$.\n",
        "\n",
        "Dado um vetor de característica $x$, nós queremos prever $\\hat{y}$, que é a probabilidade de $y$ ser igual a $1$. Em outras palavras: se $x$ é uma imagem, $\\hat{y}$ nos dirá se o objeto que estamos procurando está na imagem ou não.\n",
        "\n",
        "\\begin{equation}\n",
        "    \\hat{y} = \\underbrace{P(y = 1|x)}_{0 \\leq \\hat{y} \\leq 1}\n",
        "\\end{equation}\n",
        "\n",
        "Os parâmetros de regressão logística serão: $w$, que é um vetor de dimensão $n_{x}$, e $b$, que é um número real.\n",
        "\n",
        "\\begin{equation}\n",
        "    w \\in \\mathbb{R}^{n_{x}} \\\\\n",
        "    b \\in \\mathbb{R}\n",
        "\\end{equation}\n",
        "\n",
        "Se estivessemos usando regressão linear, poderiamos declarar que $\\hat{y} = w^{t} \\times x + b$. O problema é que essa equação pode resultar em números muito maiores que 1 ou até mesmo números negativos, o que não é nada correto visto que nosso problema é de classificação binária. Devido isso, aplicamos a função sigmóide na equação anterior (chamaremos a equação anterior de $z$):\n",
        "\n",
        "\\begin{equation}\n",
        "    \\hat{y} = \\sigma(\\underbrace{w^{t} \\times x + b}_{z})\n",
        "\\end{equation}"
      ]
    },
    {
      "metadata": {
        "id": "yeuojpMkFvh9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Função sigmóide\n",
        "A função sigmóide é uma função de ativação, isso quer dizer que ela atende a dois critérios:\n",
        "1.  Deve estar ativa (próxima de $+1$) para entradas \"corretas\" e inativa (próxima a $0$) para entradas \"erradas\";\n",
        "2.  Deve ser não linear para que a rede como um todo possa representar funções não-lineares.\n",
        "\n",
        "O gráfico a seguir mostra o comportamento dela ($z$ nesse caso seria o eixo horizontal):\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png\">\n",
        "\n",
        "Podemos perceber, por exemplo, que ela cruza o eixo vertical em $0.5$.\n",
        "\n",
        "Sua equação é:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\sigma(z) = \\frac{1}{1+e^{-z}}\n",
        "\\end{equation}\n",
        "\n",
        "Podemos realizar algumas observações, como:\n",
        "1.  Se $z$ é muito grande, então $e^{-z}$ será próximo de zero. Dessa forma: $\\sigma(z) \\thickapprox \\frac{1}{1+0} \\thickapprox 1$;\n",
        "1.  Se $z$ é muito pequeno ou é um número negativo muito grande, então $e^{-z}$ será um número muito grande. Dessa forma: $\\sigma(z) \\thickapprox \\frac{1}{1+numero\\_grande} \\thickapprox 0$;"
      ]
    },
    {
      "metadata": {
        "id": "Zy-29dtFI4Il",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Função de custo\n",
        "Serve para medir o quão bem os seus parâmetros $w$ e $b$ estão perfomando no conjunto de treinamento. Consiste em ser a média da função de perda (que pode ser vista logo mais abaixo) de cada exemplo treinado.\n",
        "\\begin{equation}\n",
        "    J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m} \\mathcal{L}(\\hat{y}^{i}, y^{i}) = -\\frac{1}{m}\\sum_{i=1}^{m} \\begin{bmatrix}\n",
        "    y^{i} \\log \\hat{y}^{i} + (1-y^{i})\\log(1-\\hat{y}^{i})\n",
        "  \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "### Função de perda\n",
        "Também chamada de **função de erro**, é usada para medir o quão boa é a saída prevista $\\hat{y}$, quando o rótulo verdadeiro for $y$. Podemos definir a perda quando o algoritmo dá o resultado, $\\hat{y}$, em relação ao rótulo verdadeiro, $y$, como:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathcal{L}(\\hat{y}, y) = \\frac{1}{2}(\\hat{y}-y)^{2}\n",
        "\\end{equation}\n",
        "\n",
        "O problema dessa equação acima é que nos deparamos com o problema de otimização com múltiplas ótimos saídas, pois não será convexo. Devido a isso, nossa função de perda deverá ser:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\mathcal{L}(\\hat{y}, y) = (y \\log \\hat{y} + (1-y)\\log(1-\\hat{y}))\n",
        "\\end{equation}\n",
        "\n",
        "Essa equação acima faz sentido se observarmos dois casos: $y=0$ e $y=1$.\n",
        "\n",
        "1.  $y=1 \\Longrightarrow \\mathcal{L}(\\hat{y}, y) = - \\log \\hat{y}$, quanto mais $\\hat{y}$ se aproxima de $0$, $- \\log \\hat{y}$ cresce exponencialmente para o infinito. Porém a função sigmóide não permite que seja maior que $1$, portanto será próximo de $1$.\n",
        "1.  $y=0 \\Longrightarrow \\mathcal{L}(\\hat{y}, y) = - \\log(1-\\hat{y})$, quanto mais $\\hat{y}$ se distancia de $0$, $\\log(1-\\hat{y})$ vai para infinito. Vale notar o sinal $-$ no início da equação. Logo, como a função sigmóide não permite ser menor que $0$, $\\hat{y}$ será próximo de $0$. \n"
      ]
    },
    {
      "metadata": {
        "id": "7TCjfaggfKBi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradiente decrescente\n",
        "Trata-se de um método iterativo de otimização, com ele iremos aprender os parâmetros $w$ e $b$ do conjunto de treinamento.\n",
        "\n",
        "Inicialmente atribuímos valores para $w$ e $b$, normalmente se atribui $0$, mas também pode ser valores aleatórios. Após isso será cálculado um valor referente a $w$ e $b$ que servirá para indicar a direção da descida mais íngreme ou descer o mais rápido possível. Esse passo será repetida várias vezes até convergir ao ponto ideal ou chegue o mais próximo dele.\n",
        "\n",
        "\\begin{equation}\n",
        "      w := w - \\alpha \\frac{\\partial J(w, b)}{\\partial w} \\\\\n",
        "      b := b - \\alpha \\frac{\\partial J(w, b)}{\\partial b} \\\\\n",
        "\\end{equation}\n",
        "\n",
        "Onde:\n",
        "- $\\alpha$ - É a taxa de aprendizagem e controla o quão grande será o passo tomado em cada iteração do gradiente decrescente\n",
        "- $\\frac{\\partial J(w, b)}{\\partial w}$ - Derivada de $w$, representa a atualização ou mudança desejada no parâmetro $w$. O mesmo vale para $\\frac{\\partial J(w, b)}{\\partial b}$.\n",
        "\n",
        "Vale lembrar que a derivada é a inclinação de uma função no ponto. Dessa forma:\n",
        "- Se a derivada for positiva, $w$ sobre uma subtração;\n",
        "- Se a derivada for negativa, $w$ sobre uma adição;\n",
        "- Se a derivada der zero, significa que chegamos no ponto crítico da função. Por se tratar de uma função convexa, existe apenas um único ponto crítico.\n"
      ]
    },
    {
      "metadata": {
        "id": "bY1CFW2gAwUR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Derivação\n",
        "A derivada de uma função $f(x) = y$, num ponto $x = x_{0}$, é igual ao valor da tangente trigonométrica do ângulo formado pela tangente geométrica à curva representativa de $f(x) = y$, no $x = x_{0}$, ou seja, a derivada é o coeficiente angular da reta tangente ao gráfico da função no ponto $x_{0}$. Em outras palavras, podemos dizer que a derivada em um ponto de função $f(x) = y$ representa a taxa de variação instantânea do $y$ em relação a $x$ neste ponto.\n",
        "\n",
        "### Algumas propriedades\n",
        "Agora iremos ver algumas propriedades da derivação.\n",
        "\n",
        "- i) Se $f(x) = a$, então $f'(x) = 0$\n",
        "- ii) Se $f(x) = ax$, então $f'(x) = a$\n",
        "- iii) (Regra do tombo) Se $f(x) = x^{a}$, então $f'(x) = a \\times x^{a-1}$\n",
        "- iv) (Derivada da soma) $\\begin{bmatrix}f(x) + g(x)\\end{bmatrix}' = f'(x) + g'(x)$\n",
        "- v) $[a \\times f(x)]' = a \\times f'(x)$\n",
        "- vi) (Regra do produto) $[f(x) \\times g(x)]' = f(x)' \\times g(x) + f(x) \\times g(x)'$\n",
        "- vii) (Regra do quociente) $\\begin{bmatrix}\\frac{f(x)}{g(x)}\\end{bmatrix}'\n",
        "= \\frac{f'(x) \\times g(x) - f(x) \\times g'(x)}{\\begin{bmatrix}g(x)\\end{bmatrix}^{2}}$\n",
        "\n",
        "### Algumas derivadas básicas\n",
        "\n",
        "- Derivada de uma constante: $\\frac{d}{dx}(c) = 0$\n",
        "- Derivada de soma/subtração: $\\frac{d}{dx}(z \\pm w) = \\frac{dz}{dx} \\pm \\frac{dw}{dx}$\n",
        "- Produto por uma constante: $\\frac{d}{dx}(c \\times w) = c\\frac{dw}{dx}$\n",
        "- Derivada do produto: $\\frac{d}{dx}(z \\times w) = z\\frac{dw}{dx} + w\\frac{dz}{dx}$\n",
        "- Derivada da divisão:  $\\frac{d}{dx}(\\frac{z}{w})\n",
        "= \\frac{w\\frac{dz}{dx} - z\\frac{dw}{dx}}{w^{2}}$\n",
        "- Derivada de $ln$: $f(x) = ln(x)$, então $f'(x) = \\frac{1}{x}$"
      ]
    },
    {
      "metadata": {
        "id": "WVCe6i7qmVEy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G84h-vcwmVNL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}